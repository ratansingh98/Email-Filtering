{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Neccessary Libraries\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim import models, corpora\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp= pd.read_csv(\"Data/newdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps =PorterStemmer()\n",
    "SplitIntoWords = lambda message: TextBlob(message.lower()).words\n",
    "WordsIntoBaseForm = lambda message: [ps.stem(word) for word in SplitIntoWords(message)]\n",
    "data = []\n",
    "for i in range(len(temp)):\n",
    "    sub_str= WordsIntoBaseForm(temp.text[i])\n",
    "    data.append(\" \".join(sub_str))\n",
    "    \n",
    "# Lets consider only 20 message for now\n",
    "data = data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 contributing words to each topic:\n",
      "\n",
      "Topic 0\n",
      "\"price\" ==> 0.1%\n",
      "\"ga\" ==> 0.1%\n",
      "\"microturbin\" ==> 0.1%\n",
      "\"deal\" ==> 0.1%\n",
      "\"custom\" ==> 0.1%\n",
      "\n",
      "Topic 1\n",
      "\"enron\" ==> 9.6%\n",
      "\"ect\" ==> 3.7%\n",
      "\"na\" ==> 3.0%\n",
      "\"hou\" ==> 2.2%\n",
      "\"phillip\" ==> 2.2%\n",
      "\n",
      "Topic 2\n",
      "\"posit\" ==> 5.5%\n",
      "\"ect\" ==> 2.7%\n",
      "\"deal\" ==> 1.8%\n",
      "\"consolid\" ==> 1.5%\n",
      "\"manag\" ==> 1.5%\n",
      "\n",
      "Topic 3\n",
      "\"ga\" ==> 2.3%\n",
      "\"price\" ==> 1.7%\n",
      "\"microturbin\" ==> 1.5%\n",
      "\"san\" ==> 1.3%\n",
      "\"com\" ==> 1.3%\n",
      "\n",
      "Topic 4\n",
      "\"schedul\" ==> 4.0%\n",
      "\"need\" ==> 2.1%\n",
      "\"phillip\" ==> 2.1%\n",
      "\"chang\" ==> 2.1%\n",
      "\"group\" ==> 2.1%\n",
      "\n",
      "Topic 5\n",
      "\"posit\" ==> 0.2%\n",
      "\"ect\" ==> 0.2%\n",
      "\"deal\" ==> 0.1%\n",
      "\"ga\" ==> 0.1%\n",
      "\"manag\" ==> 0.1%\n",
      "\n",
      "Topic 6\n",
      "\"ga\" ==> 0.1%\n",
      "\"posit\" ==> 0.1%\n",
      "\"deal\" ==> 0.1%\n",
      "\"price\" ==> 0.1%\n",
      "\"ect\" ==> 0.1%\n",
      "\n",
      "Topic 7\n",
      "\"posit\" ==> 0.2%\n",
      "\"ect\" ==> 0.2%\n",
      "\"project\" ==> 0.1%\n",
      "\"consolid\" ==> 0.1%\n",
      "\"hou\" ==> 0.1%\n",
      "\n",
      "Topic 8\n",
      "\"10\" ==> 2.6%\n",
      "\"30\" ==> 2.6%\n",
      "\"ani\" ==> 2.6%\n",
      "\"morn\" ==> 2.6%\n",
      "\"11\" ==> 2.6%\n",
      "\n",
      "Topic 9\n",
      "\"posit\" ==> 0.2%\n",
      "\"ect\" ==> 0.2%\n",
      "\"click\" ==> 0.2%\n",
      "\"hou\" ==> 0.1%\n",
      "\"save\" ==> 0.1%\n",
      "\n",
      "Topic 10\n",
      "\"posit\" ==> 0.2%\n",
      "\"ga\" ==> 0.1%\n",
      "\"deal\" ==> 0.1%\n",
      "\"ect\" ==> 0.1%\n",
      "\"price\" ==> 0.1%\n",
      "\n",
      "Topic 11\n",
      "\"posit\" ==> 0.2%\n",
      "\"project\" ==> 0.1%\n",
      "\"ect\" ==> 0.1%\n",
      "\"deal\" ==> 0.1%\n",
      "\"hou\" ==> 0.1%\n",
      "\n",
      "Topic 12\n",
      "\"posit\" ==> 0.2%\n",
      "\"ect\" ==> 0.2%\n",
      "\"physic\" ==> 0.2%\n",
      "\"deal\" ==> 0.2%\n",
      "\"abil\" ==> 0.1%\n",
      "\n",
      "Topic 13\n",
      "\"phillip\" ==> 2.1%\n",
      "\"ha\" ==> 2.1%\n",
      "\"make\" ==> 2.1%\n",
      "\"tim\" ==> 2.1%\n",
      "\"plea\" ==> 2.1%\n",
      "\n",
      "Topic 14\n",
      "\"meet\" ==> 2.9%\n",
      "\"click\" ==> 2.9%\n",
      "\"busi\" ==> 2.4%\n",
      "\"save\" ==> 2.4%\n",
      "\"would\" ==> 1.8%\n",
      "\n",
      "Topic 15\n",
      "\"posit\" ==> 0.1%\n",
      "\"project\" ==> 0.1%\n",
      "\"manag\" ==> 0.1%\n",
      "\"ect\" ==> 0.1%\n",
      "\"deal\" ==> 0.1%\n",
      "\n",
      "Topic 16\n",
      "\"255\" ==> 3.7%\n",
      "\"90\" ==> 2.5%\n",
      "\"64\" ==> 2.5%\n",
      "\"ip\" ==> 2.5%\n",
      "\"216\" ==> 2.5%\n",
      "\n",
      "Topic 17\n",
      "\"ect\" ==> 11.0%\n",
      "\"hou\" ==> 5.8%\n",
      "\"enron\" ==> 5.2%\n",
      "\"meet\" ==> 1.6%\n",
      "\"corp\" ==> 1.6%\n",
      "\n",
      "Topic 18\n",
      "\"go\" ==> 2.8%\n",
      "\"success\" ==> 2.8%\n",
      "\"way\" ==> 2.8%\n",
      "\"test\" ==> 2.8%\n",
      "\"meet\" ==> 0.1%\n",
      "\n",
      "Topic 19\n",
      "\"phillip\" ==> 2.7%\n",
      "\"million\" ==> 2.7%\n",
      "\"paula\" ==> 2.7%\n",
      "\"fine\" ==> 2.7%\n",
      "\"35\" ==> 2.7%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Processor function for tokenizing, removing stop\n",
    "# words, and stemming\n",
    "def process(input_text):\n",
    "    # Create a regular expression tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # Create a Snowball stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    # Get the list of stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Tokenize the input string\n",
    "    tokens = tokenizer.tokenize(input_text.lower())\n",
    "\n",
    "    # Remove the stop words\n",
    "    tokens = [x for x in tokens if not x in stop_words]\n",
    "\n",
    "    # Perform stemming on the tokenized words\n",
    "    tokens_stemmed = [stemmer.stem(x) for x in tokens]\n",
    "\n",
    "    return tokens_stemmed\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    # Create a list for sentence tokens\n",
    "    tokens = [process(x) for x in data]\n",
    "\n",
    "    # Create a dictionary based on the sentence tokens\n",
    "    dict_tokens = corpora.Dictionary(tokens)\n",
    "\n",
    "    # Create a document-term matrix\n",
    "    doc_term_mat = [dict_tokens.doc2bow(token) for token in tokens]\n",
    "\n",
    "    # Define the number of topics for the LDA model\n",
    "    num_topics = len(data)\n",
    "\n",
    "    # Generate the LDA model\n",
    "    ldamodel = models.ldamodel.LdaModel(doc_term_mat,\n",
    "            num_topics=num_topics, id2word=dict_tokens, passes=int(len(data)))\n",
    " \n",
    "    num_words = 5\n",
    "    print('\\nTop ' + str(num_words) + ' contributing words to each topic:')\n",
    "    for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
    "        print('\\nTopic', item[0])\n",
    "        \n",
    "        # Print the contributing words along with their relative contributions\n",
    "        list_of_strings = item[1].split(' + ')\n",
    "        for text in list_of_strings:\n",
    "            weight = text.split('*')[0]\n",
    "            word = text.split('*')[1]\n",
    "            print(word, '==>', str(round(float(weight) * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
